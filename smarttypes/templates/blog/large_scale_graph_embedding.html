
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:py="http://genshi.edgewall.org/"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    lang="en">

<xi:include href="../master.html" />      
<head>

</head>                    

<body>

<h2>Large scale graph embedding</h2>

<p>
It's a simple idea, but it took this paper to get me to go for it: 
</p>

<ul>
<ol>
<a href="http://research.google.com/pubs/archive/36923.pdf">
Large-Scale Community Detection on YouTube for Topic Discovery and Exploration
</a>
</ol>
</ul>

<p>
Here's a brief synopsis:
</p>

<p>
Large scale community detection is hard.  
'Global methods' require knowledge of the entire graph, 
which is practically impossible for web scale graphs.
'Local methods' get stuck in a local optima.  
This paper suggests a multi-stage approach:
</p>

<ul>
<li>Step 1: Seed node identification (simple graph partitioning)</li>
<li>Step 2: Use seed node as root for distributed community detection</li>
<li>Step 3: Intelligently stitch everything together</li>
</ul>

<p>
A big difference between their implementation and 
the smarttypes implementation is:
</p>

<ul>
<ol>
We're concerned w/ community detection 
<span style="font-weight:bold;">and graph embedding</span>.
</ol>
</ul>

<p>
Here's 
<a href="http://www.smarttypes.org/blog/graph_reduction_linlog_nbody_simulation">
a little diddy about graph embedding and the linlog energy model</a>
</p>

<p>This is from 
  <a href="https://github.com/smarttypes/smarttypes/blob/master/smarttypes/graphreduce/README">
    the smarttypes graphreduce README 
  </a>:
</p>

<pre>Graphreduce is designed to identify communities (or clusters) in large 
biological networks, and map these clusters to a 2-dimensional space.

We assume the entire graph won't fit into memory, which makes for an 
interesting challenge.  We use a distributed learning algorithm to meet 
this challenge.

Here are the steps:

 1. Kick off a new reduction (log progress along the way).

 2. Intelligently select N seed nodes. Select nodes that are far apart, 2nd 
    level (degree) has little overlap w/ other seed 2nd level (it's a guess,
    not going to measure @ this point)

 3. Use N seed nodes to get N 'seed networks' -- pull in the people seed node 
    follows and the people those people follow (2 degrees) -- if each person 
    follows 100 people on average it's 10,000 nodes

 4. Measure seed node selection performance (use seed networks)

 5. Use seed networks for distributed community detection -- see 'Community 
    detection improvements' in the igraph 0.6 release notes: 
    http://igraph.sourceforge.net/relnotes-0.6.html

 6. Generate 'local' community stats (assume we only know about a single 
    community at a time -- we iterate over the communities, or do in parallel)

 7. Remove duplicate nodes (use local community stats)

 8. Embed community nodes (use local community stats)

 9. Measure community embedding performance (use local community stats + 
    community embedding)

 10. Rinse, wash, repeat -- Step 2. looks at previous reduction to select seed 
    nodes (this is the much-hyped learning bit)

Here's how it all maps to code:

  Step 1: reduction_job = start_new_reduction()

  Step 2: seed_nodes = get_seed_nodes(reduction_job)

  Step 3: seed_networks = get_seed_networks(reduction_job, seed_nodes)

  Step 4: measure_seed_node_selection_performance(reduction_job, seed_networks)

  Step 5: communities = do_distributed_community_detection(reduction_job, 
                          seed_networks)

  Step 6: community_stats = gen_community_stats(reduction_job, communities) 

  Step 7: communities = remove_dup_nodes(reduction_job, community_stats)

  Step 8: 2d_embedding = embed_community_nodes(reduction_job, community_stats)

  Step 9: measure_community_embedding_performance(reduction_job, community_stats, 
            2d_embedding)
</pre>

</body>                                    
</html>

